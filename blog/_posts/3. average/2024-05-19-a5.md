---
layout: post
title: "Арбитал — это круто"
show_title: true
medium: https://medium.com/@k.kirdan/%D0%B0%D1%80%D0%B1%D0%B8%D1%82%D0%B0%D0%BB-%D1%8D%D1%82%D0%BE-%D0%BA%D1%80%D1%83%D1%82%D0%BE-dd0508f2aea0
updated: 2025-07-03
tags: ["искусственный_интеллект", "математика", "рациональность"]
importance: 8
---
Я тут параллельно с администрированием [Reducing Suffering](https://reducingsuffering.github.io/) решил позаниматься небольшими переводами с сайта [arbital.com](https://arbital.com). Пока не знаю что из этого выйдет, но воспользуюсь моментом, чтобы порекомендовать этот ресурс.

Если вы уже знакомы с [LessWrong](https://lesswrong.ru/), то вам будет проще понять, к чему это все: Арбитал был [посвящен](https://arbital-ru.github.io/explore/ai_alignment) в первую очередь проблемам согласования искусственного интеллекта (если не понимаете что это и зачем — см. [ссылки в конце](#Дополнение)). Кроме того, там было [много](https://arbital-ru.github.io/explore/math) объясняющих материалов по математике.

Сайт задумывался как что-то среднее между Вики и платформой для блогинга. Похоже, где-то с 2017-го он [больше вообще не развивается](https://www.lesswrong.com/posts/kAgJJa3HLSZxsuSrf/arbital-postmortem), но к этому моменту было проделано уже немало работы и его до сих пор можно использовать в качестве справочника по некоторым вопросам. Например, там хорошо описаны темы вроде [ортогональности ценностей и интеллекта](https://arbital.com/p/orthogonality/) и [теории вероятностей](https://arbital.com/explore/1bv). Одним из наиболее активных авторов на сайте был Элиезер Юдковский. Если вам не нравится как он пишет, то нужно отметить, что стиль постов на Арбитале заметно отличается от стиля "[Цепочек](https://lesswrong.ru/w/%D0%A0%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%BE%D1%82_%D0%98%D0%98_%D0%B4%D0%BE_%D0%97%D0%BE%D0%BC%D0%B1%D0%B8)" на LessWrong. Возможно, это так не только из-за того, что он писал с учетом специфики проекта, но и потому, что посты в дальнейшем редактировались другими людьми — ведь там что-то типа Вики.

К сожалению, оригинальный сайт ужасно тормозит и иногда вообще не подгружает контент. И поиск там не работает. Может, эти проблемы появились уже в связи с закрытием проекта, я не в курсе. Но выход есть: все материалы можно читать через [GreaterWrong](https://arbital.greaterwrong.com/) без каких-либо тормозов. Правда, есть и недостаток: иногда GreaterWrong одним текстом вместе с контентом поста отображает и рабочие комментарии типа "to do" (впрочем, их обычно легко распознать).

Я давно хотел перевести пару важных штук с Арбитала, но на больших текстах мне сложно удерживать мотивацию, поэтому я решил начать с чего-то попроще. И вот, в течение всего нескольких дней я перевел 14 небольших постов. В частности, в этой серии постов рассматривается ряд вопросов, названных в честь Вернона Винджа, и касающихся [проблемы](https://arbital-ru.github.io/p/Vinge_principle/) [предсказания](https://arbital-ru.github.io/p/Vingean_uncertainty/) поведения более умных агентов (в том числе будущих версий вас самих). Еще, например, я переводил посты о [максимизаторе скрепок](https://arbital-ru.github.io/p/paperclip_maximizer/), об отличии [инструментальных ценностей/целей от терминальных](https://arbital-ru.github.io/p/terminal_vs_instrumental/), а также шуточный пост об [инопланетном философе-тролле](https://arbital-ru.github.io/p/omega_troll/) (которого вы раньше могли встречать в дилеммах вроде [задачи Ньюкома](https://lesswrong.ru/w/%D0%9F%D0%B0%D1%80%D0%B0%D0%B4%D0%BE%D0%BA%D1%81_%D0%9D%D1%8C%D1%8E%D0%BA%D0%BE%D0%BC%D0%B0_%D1%81%D0%BE%D0%B6%D0%B0%D0%BB%D0%B5%D1%8F_%D0%BE_%D1%81%D0%B2%D0%BE%D0%B5%D0%B9_%D1%80%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D0%B8)).

Поскольку многие посты на Арбитале связаны друг с другом ссылками и иерархическими отношениями, мне пришло в голову, что было бы неплохо не просто переводить отдельные тексты, но и попробовать воспроизвести структуру исходного сайта. [Здесь](https://arbital-ru.github.io/) можно посмотреть, что у меня получается и чем-нибудь помочь.

**Обновление от 3 июля 2025**: на данный момент я перевел уже более 70 страниц, в основном из раздела о безопасности ИИ. Короче, [забегайте почитать](https://arbital-ru.github.io/).

Вы также можете помочь развить русскоязычную [LessWrong Вики](https://lesswrong.ru/wiki/), адаптируя под нее идеи с Арбитала.

[Ян Лютнев](https://www.youtube.com/@user-sb2ny6kv9g) [хотел поснимать](https://t.me/yanlyutnev/1412) для YouTube ролики с озвучиванием текстов по безопасности ИИ  — вроде тех, что я перевожу —  но похоже, со временем он отказался от этой идеи.

<a id="Дополнение"></a>
## Дополнение

Почему вам может быть интересна тема согласования ИИ? Предлагаю начать с этих материалов:

1. Келси Пайпер, [Почему стоит работать над вопросами безопасности искусственного интеллекта уже сейчас](https://ea-ru.org/articles/artificial-intelligence-research)
2. Дэн Хендрикс и другие, [Обзор катастрофических рисков ИИ](https://lesswrong.ru/%D0%9E%D0%B1%D0%B7%D0%BE%D1%80_%D0%BA%D0%B0%D1%82%D0%B0%D1%81%D1%82%D1%80%D0%BE%D1%84%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D1%85_%D1%80%D0%B8%D1%81%D0%BA%D0%BE%D0%B2_%D0%98%D0%98)
3. Люк Мюльхаузер, [Перед лицом сингулярности](https://intelligenceexplosion.com/ru/)
4. Ник Бостром, [Искусственный интеллект](https://www.mann-ivanov-ferber.ru/books/iskusstvennyj-intellekt/) (книга)
5. Андрей Ведерников, [Искусственный интеллект и экзистенциальные риски](https://reverendbayes.notion.site/c20d4ed31858464d9737665455c0dbee)
6. [Список материалов](https://lesswrong.ru/node/1058) про согласование ИИ на русскоязычном LessWrong
7. Сайт "[Безопасность искусственного интеллекта](https://aisafety.ru/)"

P.S. Если же вас интересует именно [негативно-утилитарная](https://reducingsuffering.github.io/what-is-negative-utilitarianism.html) перспектива на этот вопрос (т. е. [уменьшение страданий](https://reducingsuffering.github.io/what-is-suffering-reduction.html)), рекомендую [свою статью](414.html) на эту тему.
