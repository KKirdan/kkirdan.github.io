---
layout: post
title: "Что как по искусственному интеллекту"
show_title: true
vk: 
telegram: 
tags: ["искусственный_интеллект"]
importance: 7
excerpt: "Перечисляю здесь свои успехи и приблизительные дальнейшие планы по переводам и написанию текстов по тематике искусственного интеллекта, а также некоторые мысли и рекомендации. Затронул вопросы безопасности и сроков для общего ИИ, связанных с ИИ рисков астрономических страданий, ортогональности целей и интеллекта, когерентного экстраполированного воления, и освещения подобных тем на YouTube."
---
Перечисляю здесь свои успехи и приблизительные дальнейшие планы по переводам и написанию текстов по тематике искусственного интеллекта, а также некоторые мысли и рекомендации.

Затронул вопросы безопасности и сроков для общего ИИ, связанных с ИИ рисков астрономических страданий, ортогональности целей и интеллекта, когерентного экстраполированного воления, и освещения подобных тем на YouTube.

## Безопасность ИИ в целом

В конце этой весны я [начал](https://kkirdan.github.io/blog/a5.html) переводить материалы с сайта [Arbital](https://arbital.com/), посвященного теме согласования ИИ (во всяком случае с позиции [MIRI](https://ru.wikipedia.org/wiki/Machine_Intelligence_Research_Institute)) и математике. Сайт очень интересный и давно привлекал мое внимание. Я [подготовил](https://arbital-ru.github.io/) уже более 50-ти страниц с Арбитала и перенес ту часть навигации, которая покрывает основные материалы по ИИ. Планирую как минимум еще один такой марафон по переводам, но пока еще не знаю когда. Сейчас у меня лежит кучка неоконченных черновых. Помимо своих переводов я также отредактировал парочку чужих, и еще три нужно будет проверить.

Хочу поблагодарить [Максима Выменца](https://t.me/makikoty) и других, кто помогал мне с вычиткой переводов. Мак периодически переводит статьи самых разных авторов на тему безопасности ИИ и выкладывает на сайте [aisafety.ru](https://aisafety.ru/) и на [русском LessWrong](https://lesswrong.ru/) — рекомендую ознакомиться. Он переводил, например, очень понравившийся мне «[Обзор катастрофических рисков ИИ](https://lesswrong.ru/%D0%9E%D0%B1%D0%B7%D0%BE%D1%80_%D0%BA%D0%B0%D1%82%D0%B0%D1%81%D1%82%D1%80%D0%BE%D1%84%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D1%85_%D1%80%D0%B8%D1%81%D0%BA%D0%BE%D0%B2_%D0%98%D0%98)», цепочку о «[Встроенной агентности](https://lesswrong.ru/w/%D0%92%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%B0%D0%B3%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C)», а также нашумевшую статью «[Поставить разработку ИИ на паузу не достаточно. Нам надо остановить её полностью](https://lesswrong.ru/w/%D0%9F%D0%BE%D1%81%D1%82%D0%B0%D0%B2%D0%B8%D1%82%D1%8C_%D1%80%D0%B0%D0%B7%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D1%83_%D0%98%D0%98_%D0%BD%D0%B0_%D0%BF%D0%B0%D1%83%D0%B7%D1%83_%D0%BD%D0%B5_%D0%B4%D0%BE%D1%81%D1%82%D0%B0%D1%82%D0%BE%D1%87%D0%BD%D0%BE_%D0%9D%D0%B0%D0%BC_%D0%BD%D0%B0%D0%B4%D0%BE_%D0%BE%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%B8%D1%82%D1%8C_%D0%B5%D1%91_%D0%BF%D0%BE%D0%BB%D0%BD%D0%BE%D1%81%D1%82%D1%8C%D1%8E)» Элиезера Юдковского.

Не так давно я открыл для себя такого автора как Роман Ямпольский, у которого [огромное](https://www.researchgate.net/profile/Roman-Yampolskiy) количество статей, касающихся ИИ. На русском языке есть лишь две из них («[От зачаточного ИИ к Технической Сингулярности](https://22century.ru/popular-science-publications/to-singularity)» и «[Герметизация сингулярности. Проблема ограничения свободы искусственного интеллекта ](https://22century.ru/popular-science-publications/leakproofing-the-singularity)»). А в прочих статьях он затрагивал, например, проблемы [необъяснимости, непредсказуемости и неконтролируемости](https://lenta.ru/news/2024/02/13/ai-control/) общего ИИ и [вопросы благополучия](https://www.mdpi.com/2504-2289/3/1/2) обладающих сознанием ИИ-систем. Может быть когда-нибудь что-нибудь переведу, но пока в планах хватает и других авторов.

## Риски страданий, связанные с ИИ

Сейчас нахожусь в процессе переписывания своей [старой статьи](https://kkirdan.github.io/blog/414.html) об ИИ в контексте страданий. Одна из ключевых ее тем — это риски будущих астрономических страданий ([s-риски](https://centerforreducingsuffering.org/research/intro/)). Я планирую серию переводов (в основном статей Тобайаса Баумана) на эту тему в целом, не только про ИИ. Из переведенного уже можно почитать «[Важность далекого будущего](https://reducingsuffering.github.io/tobias-baumann-the-importance-of-the-far-future.html)» Баумана и [ранее публиковавшиеся переводы других авторов](https://reducingsuffering.github.io/tags.html#s-%D1%80%D0%B8%D1%81%D0%BA%D0%B8) на Reducing Suffering.

Среди уже переведенных с Арбитала материалов я особенно хочу отметить пару статей, посвященных одному из возможных сценариев s-рисков: «[Ментальные преступления](https://arbital-ru.github.io/p/mindcrime/)» и «[Введение в ментальные преступления](https://arbital-ru.github.io/p/mindcrime_introduction/)» описывают сценарии, в которых могут в больших количествах страдать потенциально морально значимые подсистемы в крупных ИИ-системах (по причине, например, излишне детальных симуляций других агентов в процессе мышления ИИ).

Еще я примерно наполовину отредактировал перевод замечательной статьи «[Artificial Suffering: An Argument for a Global Moratorium on Synthetic Phenomenology](https://www.worldscientific.com/doi/10.1142/S270507852150003X)» Томаса Метцингера, посвященной s-риску, связанному с возможностью страданий у самих ИИ-систем в целом, а также другим вытекающим из него рискам (он также затрагивал эту тему в книге «[Наука о мозге и миф о своем Я. Тоннель Эго](https://batrachos.com/sites/default/files/pictures/Books/Mettsinger_2016_Nauka%20o%20mozge%20i%20mif%20o%20svoem%20ya.pdf)»). Как и Юдковский, он предлагает ввести всемирный мораторий на некоторые виды разработок до тех пор, пока нельзя будет значительно снизить риски. (Я скептично отношусь к возможности подобных мораториев в ближайшее время.)

В дальнейшем также планирую перевести с Арбитала небольшую [статью](https://arbital.com/p/hyperexistential_separation/) о гиперэкзистенциальных рисках, которые сильно пересекаются с s-рисками.

## Ортогональность целей и интеллекта

Давно хочу осветить тему ортогональности (независимости) целей и интеллекта. Если огрубить, то тезис ортогональности утверждает, что теоретически возможны почти любые сочетания (выполнимых) целей и интеллекта. В частности, возможны сверхинтеллектуальные существа, занимающиеся полной ерундой, например [максимизирующие число канцелярских скрепок](https://arbital-ru.github.io/p/paperclip_maximizer/) во вселенной.

На русском об этом можно [немного почитать](https://lenta.ru/articles/2016/03/06/superintellect/) в книге Ника Бострома «Искусственный интеллект. Этапы. Угрозы. Стратегии». Но наиболее ценные материалы на эту тему пока не переведены — в первую очередь это [статья](https://arbital.com/p/orthogonality/) с Арбитала (за которую я, наверное, возьмусь при следующем переводческом забеге). Я также собирал другие материалы по теме, могу порекомендовать «[General Purpose Intelligence: Arguing the Orthogonality Thesis](https://www.fhi.ox.ac.uk/wp-content/uploads/Orthogonality_Analysis_and_Metaethics-1.pdf)» и «[An AGI Modifying Its Utility Function in Violation of the Strong Orthogonality Thesis](https://www.mdpi.com/2409-9287/5/4/40)».

Помимо пересказа темы, мне хотелось бы поразмышлять над тем, 1) какие возможны более скромные утверждения о структуре [пространства возможных умов](https://arbital-ru.github.io/p/mind_design_space_wide/), достаточно релевантные для безопасности ИИ, и 2) что означало бы для нас гипотетическое серьезное нарушение ортогональности, и какие причины могли бы его вызвать. Последняя тема частично затронута в моем [черновике](https://kkirdan.github.io/blog/414.html) статьи об ИИ и страданиях.

Недавно я высказал опасение о том, что [моральный реализм](https://insolarance.com/moral-realism/) (во всяком случае его жесткие формы, подразумевающие [интернализм мотивации](https://plato.stanford.edu/entries/moral-motivation/)) может быть препятствием на пути понимания некоторых тем в сфере ИИ, таких как ортогональность. Пока сложно понять насколько велико это препятствие, но [статистика](https://survey2020.philpeople.org/) о том, сколько философов придерживаются морального реализма, меня несколько пугает (впрочем, далеко не все из них интерналисты в отношении мотивации и непонятно, как бы они на самом деле отнеслись к теме, так что вопрос требует уточнений). Но, похоже, это говорит в пользу того, что выбор позиции в метаэтике далек от вкусовщины — он может иметь очень значительные последствия в приоритизации наших усилий на практике.

## Когерентное экстраполированное воление

Что бы мы сочли правильным, если бы у нас было больше знаний, если бы мы лучше понимали и контролировали самих себя, и если бы мы рассматривали как можно больше разных аргументов в попытке выработать общее для всех моральное решение? Примерно так можно кратко описать идею когерентного экстраполированного воления (CEV), предложенную Элиезером Юдковским в качестве [цели согласования](https://arbital.com/p/cev/) для [автономного](https://arbital-ru.github.io/p/Sovereign/) ИИ общего назначения, а также в качестве [этической теории](https://arbital.com/p/normative_extrapolated_volition/). Рассказать об этой идее я хотел еще в прошлом году, но так пока и не добрался. 

Тем, кто не читал (пока в основном непереведенную) [цепочку](https://www.lesswrong.com/tag/metaethics-sequence) Юдковского о метаэтике, может быть интересно узнать, что Юдковский считал себя моральным реалистом и [отвергал](https://www.lesswrong.com/posts/RBszS2jwGM4oghXW4/the-bedrock-of-morality-arbitrary) обвинения в релятивизме. Однако, в то же время он учитывал ортогональность, отвергая возможность существования [универсально убедительных](https://reducingsuffering.github.io/eliezer-yudkowsky-no-universally-compelling-arguments.html) философских аргументов и [моральный интернализм](https://plato.stanford.edu/entries/moral-motivation/). А результат предлагаемого им процесса экстраполяции может сильно зависеть от переменчивой обстановки в сфере человеческих ценностей, и наблюдающиеся [глубокие ценностные разногласия](https://magnusvinding.com/2018/12/14/is-ai-alignment-possible/) между людьми порождают вопросы к его осуществимости и устойчивости. Поэтому ряд комментаторов настаивали, что подход Юдковского к морали все же [релятивистский](https://kkirdan.github.io/blog/253.html). Конечно, этот спор может оказаться просто [терминологическим](https://lesswrong.ru/w/%D0%A1%D0%BF%D0%BE%D1%80%D1%8B_%D0%BE%D0%B1_%D0%BE%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F%D1%85) и порождаться конфликтом интуиций о том, какое содержание следует вкладывать в слово "релятивизм".

А так, конечно, идея красивая. В контексте философии морали ее сравнивают, например, с [теорией идеального наблюдателя](https://intelligence.org/files/IdealAdvisorTheories.pdf) и методом [рефлективного равновесия](https://brickofknowledge.com/articles/reflective-equilibrium).

## Сроки

Я не особенно разбирался в теме [сроков](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) появления тех или иных видов ИИ, и желания углубляться в этот вопрос у меня пока нет.

Когда-то давно я немного читал о [взрывном](https://old-wiki.lesswrong.com/wiki/AI_takeoff#Hard_takeoff) сценарии развития ИИ в «[Перед лицом Сингулярности](https://intelligenceexplosion.com/ru/)» Люка Мюлхаузера и где-то у Элиезера Юдковского, а также слышал о прогнозах Рэя Курцвейла (хотя читал у него я больше про другое — про [вопросы архитектуры](https://www.koob.ru/kurzweil/evolyutsiya_razuma) мозга и ИИ).

А недавно я перевел пост Магнуса Виндинга со [списком источников](https://reducingsuffering.github.io/magnus-vinding-a-contra-ai-foom-reading-list.html), критикующих возможность взрывного развития ИИ. К сожалению, лишь некоторые пункты из его списка удалось найти на русском языке (зато среди них есть книги). Но возможно я переведу что-нибудь еще оттуда.

## Освещение безопасности ИИ на YouTube

Сам я освещением этих тем на YouTube не занимаюсь, но этим занимаются, например, [Минимизаторы Скрепок](https://www.youtube.com/@miniclipy). В частности,
[Ян Лютнев](https://t.me/yanlyutnev) ([YouTube](https://www.youtube.com/@user-sb2ny6kv9g/videos)), уже озвучивший часть переведенных мной материалов с Арбитала. Так вот, Ян [запрашивает помощь](https://t.me/yanlyutnev/1537) со сценариями и оформлением.

А что еще есть на русскоязычном YouTube на такие темы? Я видел [переводы роликов](https://www.youtube.com/playlist?list=PL8YZyma552VeTeGYUfnBNhCXvMPggD8yy) Роберта Майлза, сделанные студией Vert Dider, перевод интервью с Юдковским («[Элиезер Юдковский про ИИ, клубнику, банк спермы, режим бога и лучших ученых на острове](https://www.youtube.com/watch?v=fQ9fxZNjqMk)»), а также неплохое [видео](https://www.youtube.com/watch?v=fJOPGbbqMvw) у ALI (в основу которого легла статья «Искусственный интеллект как позитивный и негативный фактор глобального риска» Юдковского). Наверняка есть много чего еще, но я не особо слежу за темой.

А вот об s-рисках, к сожалению, ничего нет. Но недавно [Rational Animations](https://www.youtube.com/@RationalAnimations) сняли [красивый ролик](https://www.youtube.com/watch?v=fqnJcZiDMDo) на эту тему. Студия Vert Dider уже переводила и озвучивала [некоторые их ролики](https://www.youtube.com/playlist?list=PL8YZyma552VfzEtx3cK4cav2LmEGZ7N7q), может и этот когда-нибудь будет? В любом случае можно было бы перевести субтитры для самого канала Rational Animations.

## Взаимодействие с ИИ

Эта тема и так освещается множеством источников, а я довольно медленно вкатываюсь в новые технологии, так что мне в любом случае особо нечем поделиться. Ну, когда-то я немножко [писал](https://kkirdan.github.io/blog/478.html) о такой штуке как Replika AI. А сейчас вот потихоньку осваиваю ChatGPT, он мне нравится.
